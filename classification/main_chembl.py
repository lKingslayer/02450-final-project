# -*- coding: utf-8 -*-
"""sham_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxskzzCCOBkI5wILmi-BE6fjZ-PFLvf0
"""

import pandas as pd
import numpy as np
import os
import pickle

import matplotlib.pyplot as plt
from sklearn.base import clone
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

"""# Loading and previewing data"""

ROOTDIR = os.getcwd()
fileName = "Full_Data_Automation.pkl"

"""# Define helper functions"""

from sklearn.model_selection import GroupShuffleSplit


def gshuffle_cross_validator(lossFn, X, y, groups, seed, test_size=0.2, numFold=5):

  gss = GroupShuffleSplit(
      n_splits=numFold, test_size=test_size, random_state=seed)
  scores = []
  for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):
    X_test = X[test_index, :]
    y_test = y[test_index]

    X_train = X[train_index, :]
    y_train = y[train_index]

    # Train classifier and evaluate
    clf = RandomForestClassifier(
        random_state=seed, n_estimators=1000, n_jobs=8)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    score = lossFn(y_test, y_pred)
    scores.append(score)
  avg_score = np.mean(scores)
  return avg_score


def data_splitter(data_length, fold):
  '''
  Takes data length and splits it according to given proportion into training and test sets.
  Args:
    data_length (int): The number of samples in the data (N)
    train_proportion (float): A value between 0 and 1 representing the proportion of N which will be used for training.
  Output:
    fold_indices (array): A (fold x n) array such that n = data_length / fold.
  '''
  data = {}
  # Get the number of samples in each fold (number of test samples in each fold; remaining will be training)
  numTest = int(data_length / fold)
  allInds = list(range(data_length))
  # Split the data into random folds without replacement
  fold_indices = np.random.choice(allInds, size=(fold, numTest), replace=False)

  return fold_indices


def k_fold_cross_validator(lossFn, model, numFold, X_data, y_data):
  scores = []
  data_length = X_data.shape[0]
  allInds = list(range(data_length))
  # Generates (5 x pretrain_size/5) array of indices for each fold. Note: these indices are w.r.t. the pretrain array, NOT original data array
  k_fold_indices = data_splitter(data_length, numFold)

  for fold in range(numFold):
    # Obtain test set of data
    test_fold = k_fold_indices[fold, :]
    X_test = X_data[test_fold, :]
    y_test = y_data[test_fold]

    # Obtain training set of data
    train_fold = [i for i in allInds if i not in test_fold]
    X_train = X_data[train_fold, :]
    y_train = y_data[train_fold]

    # Train classifier and evaluate
    clf = RandomForestClassifier(
        random_state=s, n_estimators=1000, n_jobs=8)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    score = lossFn(y_test, y_pred)
    scores.append(score)
  avg_score = sum(scores) / len(scores)
  return avg_score


def query_by_committee(ensemble, X, batch_size):
  '''
  Takes a committee of models, quantify committee disagreement and pick the next data point to sample.
  Args:
    ensemble (sklearn model): E.g. RFClassifier. The individual models should be accessible from this object. It could also be a list of model objects.
    X (np array): UNLABELLED data.
  Output:
    xIdx (int): the row index of the selected data point that causes most disagreement.
  '''
  n_models = len(ensemble)
  n_points = X.shape[0]
  resultMatrix = []

  if n_points == 1:
    return 0

  def predict_model(model, X):
    return model.predict_proba(X)

  # Parallelize the model predictions
  resultMatrix = Parallel(n_jobs=-1)(
      delayed(predict_model)(model, X) for model in ensemble
  )

  # We will get a matrix of size n_models x n_points x n_classes of probabilities
  resultMatrix = np.array(resultMatrix)
  # We get the average probability of a label of any given point across all models --> n_points x n_classes array
  consensusScores = np.sum(resultMatrix, axis=0) / n_models

  # Calculate KL-Divergence:
  # 1. probability of label for each model is divided by probability of a label in consensus
  # Add a small value in case denominator is 0
  kld = resultMatrix / (consensusScores + 0.00001)

  # 2. take the log of this ratio
  # Add a small value so log doesn't blow up to inf
  kld = np.log(kld + 0.00001)

  # 3. element-wise multiply the ratio by probability of label of each model
  kld = resultMatrix * kld

  # 4. take the sum of all labels for every model
  kld = np.sum(kld, axis=(0, 2))  # we should get 1 x n_points array

  # Pick index of point with largest value
  sortedIdx = np.argsort(kld)
  batch = sortedIdx[-batch_size:]

  return batch


def uncertainty_sampling(trainedModel, X, batch_size=1):
  '''
  Takes a trained model and returns the index of the most uncertain data point in the unseen dataset using entropy.

  Args:
    trainedModel (sklearn.model object): a model that has already been fit on training data.
    X (array): a n' x k matrix of unseen observations where n' is the number of observations and k is the number of features.
    batch_size (int): Number of points to return

  Output:
    chosenIndex (np array of int): batch_size number of indices of the unseen data that has the highest entropy.
  '''
  y_pred_prob = trainedModel.predict_proba(
      X)  # predict probabilities of each class at each data point
  # Find the log probabilities of each class at each data point
  log_prob = np.log(y_pred_prob)
  # multiply probabilities by log probabilities element-wise
  summands = np.multiply(y_pred_prob, log_prob)
  # Take sum of all labels at every row and take negative of sum to get entropy
  entropy = (-1) * np.sum(summands, axis=1)
  sorted_entropy = np.argsort(entropy)
  return sorted_entropy[-batch_size:]


from sklearn.cluster import KMeans


def diversity_k_means(X, batch_size):
  '''
  Takes input data and selects batch_size number of points through k-means clustering, with k=batch_size. Selects points closest to each cluster centroid
  Args:
    X (array): a n' x k matrix of unseen observations where n' is the number of observations and k is the number of features.
    batch_size (int): Number of points to return.

  Outputs:
    chosenIndex (np array of int): batch_size number of indices of the unseen data that has the highest entropy.
  '''
  cls = KMeans(n_clusters=batch_size, n_init=10)
  kmeans = cls.fit(X)
  # Get cluster labels for each data point, (n_samples,)
  labels = kmeans.predict(X)
  # Get euclidean distance of each point from every cluster centre, (n_samples, batch_size)
  distances = kmeans.transform(X)

  # cluster_min=np.argmin(distances,axis=0) # Find the row index of the closest point to each cluster centre (column). (batch_size,) --> Causes some repeats in batch
  minInds = []

  while len(minInds) < batch_size:
    # the minimum distance in each centroid
    allMins = np.min(distances, axis=0)
    # sorts the clusters in ascending order of minimum distance
    sortedMins = np.argsort(allMins)
    minCentre = sortedMins[0]
    # find minimum in each cluster going from smallest to largest
    minIdx = np.argmin(distances[:, minCentre])
    minInds.append(minIdx)
    # Remove sample from being considered in other clusters
    distances = np.delete(distances, minIdx, axis=0)
    # Remove cluster centroid from being considered
    distances = np.delete(distances, minCentre, axis=1)

    if distances.shape[0] == 1:
      break
    elif distances.shape[1] == 0:
      break

  return minInds


"""# Write model runner function"""

from joblib import Parallel, delayed


def model_runner_kfold(dataDir, seeds, scoreFn, queryMethod=None, batch_size=None, shuffle=None, embedding=None, start_prop=0.1, end_prop=0.4):
  '''
  Highest-order function which trains a Random Forest Classifier on training data.
  Args:
    dataDir (str): absolute path to data to be read as a DataFrame. Should contain protein and ligand embeeddings, and target variable for regression.
    seeds (list of int): seeds to initialize dataset and train model.
    queryMethod (str): choices = ["None", "Random", "Uncertainty", "Diversity", "QBC"]. Default=None (runs CV on entire dataset, i.e. offline model)
    batch_size (int): determines number of samples queried in each iteration. Default=None (offline model)
    shuffle (str): choices = ["None", "Protein", "Ligand", "Label"]. Default=None (no shuffling done on embeddings)
    embedding (str): choices = ["None", "Protein", "Ligand"]. Default=None (both protein and ligand are used). This setting only confirms result from shuffle so no need to change.
    start_prop (float): Value in range (0,1) that determines how much of the dataset is used for initialization. Default=0.2
    end_prop (float): Value in range (start_prop,1] that determines how much of the dataset is used for terminating AL. Default=0.5
  Outputs:
    CV_scores_mean, CV_scores_std, Classif_scores_mean, Classif_scores_std (np arrays): each is a (numIterations,)-length array of either cross-validation or testing metric, taken as mean or std across number of seeds used.
  '''
  # 1. Read in the data and obtain embedding vectors
  data = pd.read_pickle(dataDir)
  ligand = np.vstack(data["LIGAND_EMBEDDING"].to_numpy())
  protein = np.vstack(data["PROTEIN_EMBEDDING"].to_numpy())
  y = data["ACTIVITY_STATUS"].to_numpy()
  groups = data["1_Group_x"].to_numpy()
  print(
      f"ligand size: {ligand.shape} | protein size: {protein.shape} | groups size: {groups.shape} | label size: {y.shape}")

  # If performing permutation test
  if shuffle == "Protein":
    protein = np.random.permutation(protein)
  elif shuffle == "Ligand":
    ligand = np.random.permutation(ligand)
  elif shuffle == "Label":
    y = np.random.permutation(y)

  # If only using one embedding or the other
  if embedding == "Protein":
    X = protein
  elif embedding == "Ligand":
    X = ligand
  else:
    X = np.concatenate((protein, ligand), axis=1)
  print(f"Input size: {X.shape} | Target size: {y.shape}")

  assert end_prop > start_prop
  # Select 20% of data for pretraining the model
  data_length = X.shape[0]  # Find total number of samples
  # Create a list of all indices in the sample
  allInds = list(range(data_length))
  # Find the number of samples that should be in the pretraining set
  pretrain_size = int(data_length * start_prop)
  # this will be the stop criterion for the training
  stop_size = int(end_prop * data_length)

  def single_seed(s, X, y, groups, scoreFn):
    np.random.seed(s)

    if queryMethod == None:  # Run offline model on end_prop of dataset
      randInds = np.random.choice(allInds, size=stop_size, replace=False)
      X_data = X[randInds, :]
      y_data = y[randInds]
      group_data = groups[randInds]
      clf = RandomForestClassifier(
          random_state=s, n_estimators=1000, n_jobs=8)
      CVscore = gshuffle_cross_validator(
          scoreFn, X_data, y_data, group_data, s)
      print(f"Offline CV score: {CVscore} | Seed {s}")
      unseen_Inds = [i for i in allInds if i not in randInds]
      X_unseen = X[unseen_Inds, :]
      y_unseen = y[unseen_Inds]
      clf.fit(X_data, y_data)
      y_pred = clf.predict(X_unseen)
      ClassifScore = scoreFn(y_pred, y_unseen)
      print(f"Offline Classification score: {ClassifScore} | Seed {s}")

      return [CVscore], [ClassifScore]  # Terminate function

    '''
    1. Evaluate classification accuracy on start_prop of dataset.
    '''

    # Obtain a random sample of indices that form the pretraining dataset
    pretrain_indices = np.random.choice(
        allInds, size=pretrain_size, replace=False)

    X_pretrain = X[pretrain_indices, :]  # Select X for pretraining
    y_pretrain = y[pretrain_indices]  # Select y for pretraining
    groups_pretrain = groups[pretrain_indices]

    # Maintain a list of "unseen" indices of data
    pool_indices = [i for i in allInds if i not in pretrain_indices]
    X_pool = X[pool_indices, :]
    y_pool = y[pool_indices]
    groups_pool = groups[pool_indices]
    clf = RandomForestClassifier(
        random_state=s, n_estimators=1000, n_jobs=8)
    # Get scores from CV of pretraining data
    initialScore = gshuffle_cross_validator(
        scoreFn, X_pretrain, y_pretrain, groups_pretrain, s)

    '''
    2. Pretrain the classifier on start_prop of dataset and obtain classification score on unseen data.
    '''

    clf = RandomForestClassifier(
        random_state=s, n_estimators=1000, n_jobs=8)

    CV = []  # This stores cross-validation accuracy from every new instance added
    Classification = []  # This stores classification accuracy on unseen data at every iteration
    CV.append(initialScore)

    # Pretrain the model with 20% of the data and evaluate on remaining data
    clf.fit(X_pretrain, y_pretrain)
    y_pred = clf.predict(X_pool)
    score = scoreFn(y_pool, y_pred)
    Classification.append(score)
    curr_state = X_pretrain.shape[0]

    print(f"Pretrained model: CV: {initialScore}, Classification: {score}")

    '''
    3. Add datapoints using query selection method until user-defined proportion of datapoints are in dataset.
    '''

    # Pick an unseen instance using uncertainty sampling to add to training data and perform CV
    batch = 1
    while curr_state < stop_size:

      remaining = stop_size - curr_state

      if batch % 5 == 0:
        print(
            f"Seed: {s} | Progress: {curr_state}/{stop_size} | Current score: {score}")

      # Pick a data point to add to training set based on query selection method. This point is based on the index of the current matrix X_pool, not original data matrix.
      if queryMethod == "Uncertainty":
        nextPoint = uncertainty_sampling(clf, X_pool, batch_size=batch_size)

      elif queryMethod == "Diversity":
        nextPoint = diversity_k_means(X_pool, batch_size)

      elif queryMethod == "QBC":
        ensemble = clf.estimators_
        nextPoint = query_by_committee(ensemble, X_pool, batch_size)

      elif queryMethod == "Random":
        nextPoint = np.random.choice(
            list(range(X_pool.shape[0])), size=batch_size, replace=False)  # random sampling

      next_X = X_pool[nextPoint, :]
      next_y = y_pool[nextPoint]
      next_group = groups_pool[nextPoint]

      # Add to training dataset
      if len(next_X.shape) == 1:
        X_pretrain = np.append(X_pretrain, next_X.reshape((1, -1)), axis=0)
      else:
        X_pretrain = np.append(X_pretrain, next_X, axis=0)
      y_pretrain = np.append(y_pretrain, next_y)
      groups_pretrain = np.append(groups_pretrain, next_group)

      # Remove from unseen pool
      X_pool = np.delete(X_pool, nextPoint, axis=0)
      y_pool = np.delete(y_pool, nextPoint)
      groups_pool = np.delete(groups_pool, nextPoint)

      # Train model again with 5-fold CV
      score = gshuffle_cross_validator(
          scoreFn, X_pretrain, y_pretrain, groups_pretrain, s)
      CV.append(score)

      if X_pool.shape[0] == 0:
        # No more unseen observations left
        break

      # Train model on all pretraining data and evaluate on unseen data
      clf.fit(X_pretrain, y_pretrain)
      y_pred = clf.predict(X_pool)
      score = scoreFn(y_pool, y_pred)
      Classification.append(score)

      curr_state = X_pretrain.shape[0]
      batch += 1

    return CV, Classification

  results = Parallel(n_jobs=3)(delayed(single_seed)(s, X, y, groups, scoreFn)
                               for s in seeds)  # Run different seeds in parallel
  # results=[]
  # for s in seeds:
  #   res=single_seed(s)
  #   results.append(res)

  # if queryMethod == None:
  #   # Return mean and std of offline model
  #   CV_score = [results[i][0]]
  #   Classification_score = results[1]
  #   return np.mean(results), np.std(results)

  # Read results into numpy arrays

  numIterations = stop_size - pretrain_size + 1

  CV_scores = np.zeros((len(seeds), len(results[0][0])))
  Classif_scores = np.zeros((len(seeds), len(results[0][1])))

  for r in range(len(results)):
    CV_scores[r] = results[r][0]
    Classif_scores[r] = results[r][1]

  # Take mean and standard deviation for plotting

  CV_scores_mean = np.mean(CV_scores, axis=0)
  CV_scores_std = np.std(CV_scores, axis=0)

  Classif_scores_mean = np.mean(Classif_scores, axis=0)
  Classif_scores_std = np.std(Classif_scores, axis=0)

  return CV_scores_mean, CV_scores_std, Classif_scores_mean, Classif_scores_std


dataDir = os.path.join(ROOTDIR, fileName)
seeds = [42, 3, 0]

queryMethod = [None, "Random", "Uncertainty", "Diversity", "QBC"]
batch_size = [50, 100, 200]

results = []

for q in queryMethod:
  for b in batch_size:
    print(f"Running {q} for batch size {b}")
    scores = model_runner_kfold(
        dataDir, seeds, accuracy_score, queryMethod=q, batch_size=b)
    CV_scores_mean, CV_scores_std, Classif_scores_mean, Classif_scores_std = scores
    res = {"Query": q, "Batch_Size": b, "mean_CV": CV_scores_mean, "std_CV": CV_scores_std,
           "mean_test": Classif_scores_mean, "std_test": Classif_scores_std, "Shuffle": None, "Embedding": "Both", "Score": "Accuracy"}
    results.append(res)

with open("Default_Run_Accuracy.pkl", "wb") as f:
  pickle.dump(results, f)
