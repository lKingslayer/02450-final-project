# -*- coding: utf-8 -*-
"""sham_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxskzzCCOBkI5wILmi-BE6fjZ-PFLvf0
"""

import pandas as pd
import numpy as np
import os
import pickle

import matplotlib.pyplot as plt
from sklearn.base import clone
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

"""# Loading and previewing data"""

ROOTDIR = os.getcwd()
fileName = "Full_Data_Automation.pkl"
scoreFn = accuracy_score
scoreName = "Accuracy"

"""# Define helper functions"""

from sklearn.model_selection import GroupShuffleSplit


def gshuffle_cross_validator(lossFn, X, y, groups, seed, test_size=0.2, numFold=5):

  gss = GroupShuffleSplit(
      n_splits=numFold, test_size=test_size, random_state=seed)
  scores = []
  for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):
    X_test = X[test_index, :]
    y_test = y[test_index]

    X_train = X[train_index, :]
    y_train = y[train_index]

    # Train classifier and evaluate
    clf = RandomForestClassifier(
        random_state=seed, n_estimators=1000, n_jobs=12)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    score = lossFn(y_test, y_pred)
    scores.append(score)
  avg_score = np.mean(scores)
  return avg_score


def data_splitter(data_length, fold):
  '''
  Takes data length and splits it according to given proportion into training and test sets.
  Args:
    data_length (int): The number of samples in the data (N)
    train_proportion (float): A value between 0 and 1 representing the proportion of N which will be used for training.
  Output:
    fold_indices (array): A (fold x n) array such that n = data_length / fold.
  '''
  data = {}
  # Get the number of samples in each fold (number of test samples in each fold; remaining will be training)
  numTest = int(data_length / fold)
  allInds = list(range(data_length))
  # Split the data into random folds without replacement
  fold_indices = np.random.choice(allInds, size=(fold, numTest), replace=False)

  return fold_indices


def k_fold_cross_validator(lossFn, model, numFold, X_data, y_data):
  scores = []
  data_length = X_data.shape[0]
  allInds = list(range(data_length))
  # Generates (5 x pretrain_size/5) array of indices for each fold. Note: these indices are w.r.t. the pretrain array, NOT original data array
  k_fold_indices = data_splitter(data_length, numFold)

  for fold in range(numFold):
    # Obtain test set of data
    test_fold = k_fold_indices[fold, :]
    X_test = X_data[test_fold, :]
    y_test = y_data[test_fold]

    # Obtain training set of data
    train_fold = [i for i in allInds if i not in test_fold]
    X_train = X_data[train_fold, :]
    y_train = y_data[train_fold]

    # Train classifier and evaluate
    clf = RandomForestClassifier(
        random_state=s, n_estimators=1000, n_jobs=12)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    score = lossFn(y_test, y_pred)
    scores.append(score)
  avg_score = sum(scores) / len(scores)
  return avg_score


def query_by_committee(ensemble, X, batch_size):
  '''
  Takes a committee of models, quantify committee disagreement and pick the next data point to sample.
  Args:
    ensemble (sklearn model): E.g. RFClassifier. The individual models should be accessible from this object. It could also be a list of model objects.
    X (np array): UNLABELLED data.
  Output:
    xIdx (int): the row index of the selected data point that causes most disagreement.
  '''
  n_models = len(ensemble)
  n_points = X.shape[0]
  resultMatrix = []

  if n_points == 1:
    return 0

  def predict_model(model, X):
    return model.predict_proba(X)

  # Parallelize the model predictions
  resultMatrix = Parallel(n_jobs=2)(
      delayed(predict_model)(model, X) for model in ensemble
  )

  # We will get a matrix of size n_models x n_points x n_classes of probabilities
  resultMatrix = np.array(resultMatrix)
  # We get the average probability of a label of any given point across all models --> n_points x n_classes array
  consensusScores = np.sum(resultMatrix, axis=0) / n_models

  # Calculate KL-Divergence:
  # 1. probability of label for each model is divided by probability of a label in consensus
  # Add a small value in case denominator is 0
  kld = resultMatrix / (consensusScores + 0.00001)

  # 2. take the log of this ratio
  # Add a small value so log doesn't blow up to inf
  kld = np.log(kld + 0.00001)

  # 3. element-wise multiply the ratio by probability of label of each model
  kld = resultMatrix * kld

  # 4. take the sum of all labels for every model
  kld = np.sum(kld, axis=(0, 2))  # we should get 1 x n_points array

  # Pick index of point with largest value
  sortedIdx = np.argsort(kld)
  batch = sortedIdx[-batch_size:]

  return batch


def uncertainty_sampling(trainedModel, X, batch_size=1):
  '''
  Takes a trained model and returns the index of the most uncertain data point in the unseen dataset using entropy.

  Args:
    trainedModel (sklearn.model object): a model that has already been fit on training data.
    X (array): a n' x k matrix of unseen observations where n' is the number of observations and k is the number of features.
    batch_size (int): Number of points to return

  Output:
    chosenIndex (np array of int): batch_size number of indices of the unseen data that has the highest entropy.
  '''
  y_pred_prob = trainedModel.predict_proba(
      X)  # predict probabilities of each class at each data point
  # Find the log probabilities of each class at each data point
  log_prob = np.log(y_pred_prob)
  # multiply probabilities by log probabilities element-wise
  summands = np.multiply(y_pred_prob, log_prob)
  # Take sum of all labels at every row and take negative of sum to get entropy
  entropy = (-1) * np.sum(summands, axis=1)
  sorted_entropy = np.argsort(entropy)
  return sorted_entropy[-batch_size:]


from sklearn.cluster import KMeans


def diversity_k_means(X, batch_size):
  '''
  Takes input data and selects batch_size number of points through k-means clustering, with k=batch_size. Selects points closest to each cluster centroid
  Args:
    X (array): a n' x k matrix of unseen observations where n' is the number of observations and k is the number of features.
    batch_size (int): Number of points to return.

  Outputs:
    chosenIndex (np array of int): batch_size number of indices of the unseen data that has the highest entropy.
  '''
  cls = KMeans(n_clusters=batch_size, n_init=10)
  kmeans = cls.fit(X)
  # Get cluster labels for each data point, (n_samples,)
  labels = kmeans.predict(X)
  # Get euclidean distance of each point from every cluster centre, (n_samples, batch_size)
  distances = kmeans.transform(X)

  # cluster_min=np.argmin(distances,axis=0) # Find the row index of the closest point to each cluster centre (column). (batch_size,) --> Causes some repeats in batch
  minInds = []

  while len(minInds) < batch_size:
    # the minimum distance in each centroid
    allMins = np.min(distances, axis=0)
    # sorts the clusters in ascending order of minimum distance
    sortedMins = np.argsort(allMins)
    minCentre = sortedMins[0]
    # find minimum in each cluster going from smallest to largest
    minIdx = np.argmin(distances[:, minCentre])
    minInds.append(minIdx)
    # Remove sample from being considered in other clusters
    distances = np.delete(distances, minIdx, axis=0)
    # Remove cluster centroid from being considered
    distances = np.delete(distances, minCentre, axis=1)

    if distances.shape[0] == 1:
      break
    elif distances.shape[1] == 0:
      break

  return minInds


"""# Write model runner function"""

from joblib import Parallel, delayed


def model_runner_logo(dataDir, seeds, scoreFn, queryMethod=None, batch_size=None, start_prop=0.01, end_prop=0.4):
  '''
  Highest-order function which trains a Random Forest Classifier on training data. Initializes dataset using leave-one-group-out: 2 groups will be used for every AL cycle and testing is done on hold-out group to avoid data leakage. The AL cycle is repeated using each group as hold-out.
  Args:
    dataDir (str): absolute path to data to be read as a DataFrame. Should contain protein and ligand embeeddings, and target variable for regression.
    seeds (list of int): seeds to initialize dataset and train model.
    queryMethod (str): choices = ["None", "Random", "Uncertainty", "Diversity", "QBC"]. Default=None (runs CV on entire dataset, i.e. offline model)
    batch_size (int): determines number of samples queried in each iteration. Default=None (offline model)
    start_prop (float): Value in range (0,1) that determines how much of the dataset is used for initialization. Default=0.2
    end_prop (float): Value in range (start_prop,1] that determines how much of the dataset is used for terminating AL. Default=0.5
  Outputs:
    CV_scores_mean, CV_scores_std, Classif_scores_mean, Classif_scores_std (np arrays): each is a (numIterations,)-length array of either cross-validation or testing metric, taken as mean or std across number of seeds used.
  '''
  ###################
  # Read in the data and obtain embedding vectors
  ###################
  data = pd.read_pickle(dataDir)
  ligand = np.vstack(data["LIGAND_EMBEDDING"].to_numpy())
  protein = np.vstack(data["PROTEIN_EMBEDDING"].to_numpy())
  y = data["ACTIVITY_STATUS"].to_numpy()
  groups = data["1_Group_x"].to_numpy()
  uniqueGroups = list(set(groups))
  print(
      f"ligand size: {ligand.shape} | protein size: {protein.shape} | groups size: {groups.shape} | label size: {y.shape}")

  ###################
  # Defines a function to run one AL cycle.
  ###################

  def single_seed(s, X, y, groups, scoreFn):
    np.random.seed(s)

    if queryMethod == None:  # Run offline model on end_prop of dataset using hold-out strategy

      scores = {}
      scores["CV"] = {key: 0 for key in uniqueGroups}
      scores["Classif_Train"] = {key: 0 for key in uniqueGroups}
      scores["Classif_Test"] = {key: 0 for key in uniqueGroups}
      for grp in uniqueGroups:
        print(f"Hold-out: {grp}")

        # Define current hold-out set
        grp_idx = np.where(groups == grp)[0]
        hold_out_set_X = X[grp_idx, :]
        hold_out_set_y = y[grp_idx]
        hold_out_set_groups = groups[grp_idx]

        # Define current training set
        remaining_idx = [
            i for i in list(range(groups.shape[0])) if i not in grp_idx]
        remaining_set_X = X[remaining_idx, :]
        remaining_set_y = y[remaining_idx]
        remaining_set_groups = groups[remaining_idx]

        # Select 20% of data for pretraining the model
        data_length = remaining_set_X.shape[0]  # Find total number of samples
        # Create a list of all indices in the sample
        allInds = list(range(data_length))
        # Find the number of samples that should be in the pretraining set
        pretrain_size = int(data_length * start_prop)
        # this will be the stop criterion for the training
        stop_size = int(end_prop * data_length)

        # Evaluate offline model using cross-validation
        randInds = np.random.choice(
            allInds, size=stop_size, replace=False)
        X_data = remaining_set_X[randInds, :]
        y_data = remaining_set_y[randInds]
        group_data = remaining_set_groups[randInds]
        clf = RandomForestClassifier(
            random_state=s, n_estimators=1000, n_jobs=12)
        CVscore = gshuffle_cross_validator(
            scoreFn, X_data, y_data, group_data, s)
        scores["CV"][grp] = CVscore
        print(
            f"Offline CV score for hold-out group {grp}: {CVscore} | Seed {s}")

        # Evaluate offline model using unseen data in the same training groups
        unseen_Inds = [i for i in allInds if i not in randInds]
        X_unseen = remaining_set_X[unseen_Inds, :]
        y_unseen = remaining_set_y[unseen_Inds]
        clf.fit(X_data, y_data)
        y_pred = clf.predict(X_unseen)
        ClassifScore = scoreFn(y_pred, y_unseen)
        print(
            f"Offline Classification score for training groups: {ClassifScore} | Seed {s}")
        scores["Classif_Train"][grp] = ClassifScore

        # Evaluate offline model using hold-out group
        y_pred = clf.predict(hold_out_set_X)
        ClassifScore = scoreFn(y_pred, hold_out_set_y)
        print(
            f"Offline Classification score for hold-out group {grp}: {ClassifScore} | Seed {s}")
        scores["Classif_Test"][grp] = ClassifScore

      return scores  # Terminate function

    '''
    1. Evaluate classification accuracy on start_prop of dataset.
    '''

    scores = {}
    scores["CV"] = {key: [] for key in uniqueGroups}
    scores["Classif_Train"] = {key: [] for key in uniqueGroups}
    scores["Classif_Test"] = {key: [] for key in uniqueGroups}
    for grp in uniqueGroups:
      print(f"Hold-out: {grp}")
      # Define current hold-out set
      grp_idx = np.where(groups == grp)[0]
      hold_out_set_X = X[grp_idx, :]
      hold_out_set_y = y[grp_idx]
      hold_out_set_groups = groups[grp_idx]

      # Define current training set
      remaining_idx = [
          i for i in list(range(groups.shape[0])) if i not in grp_idx]
      remaining_set_X = X[remaining_idx, :]
      remaining_set_y = y[remaining_idx]
      remaining_set_groups = groups[remaining_idx]

      # Select 20% of data for pretraining the model
      data_length = remaining_set_X.shape[0]  # Find total number of samples
      # Create a list of all indices in the sample
      allInds = list(range(data_length))
      # Find the number of samples that should be in the pretraining set
      pretrain_size = int(data_length * start_prop)
      # this will be the stop criterion for the training
      stop_size = int(end_prop * data_length)
      print(f"Pretrain size: {pretrain_size} | Stop size: {stop_size}")

      # Obtain a random sample of indices that form the pretraining dataset
      pretrain_indices = np.random.choice(
          allInds, size=pretrain_size, replace=False)

      # Select X for pretraining
      X_pretrain = remaining_set_X[pretrain_indices, :]
      # Select y for pretraining
      y_pretrain = remaining_set_y[pretrain_indices]
      groups_pretrain = remaining_set_groups[pretrain_indices]

      # Maintain a list of "unseen" indices of data within the training set
      pool_indices = [i for i in allInds if i not in pretrain_indices]
      X_pool = remaining_set_X[pool_indices, :]
      y_pool = remaining_set_y[pool_indices]
      groups_pool = remaining_set_groups[pool_indices]
      clf = RandomForestClassifier(
          random_state=s, n_estimators=1000, n_jobs=12)
      # Get scores from CV of pretraining data
      initialScore = gshuffle_cross_validator(
          scoreFn, X_pretrain, y_pretrain, groups_pretrain, s)

      '''
      2. Pretrain the classifier on start_prop of dataset and obtain classification score on unseen data.
      '''

      clf = RandomForestClassifier(
          random_state=s, n_estimators=1000, n_jobs=12)

      scores["CV"][grp].append(initialScore)

      # Pretrain the model with 20% of the data and evaluate on remaining data
      clf.fit(X_pretrain, y_pretrain)
      y_pred = clf.predict(X_pool)
      trainscore = scoreFn(y_pool, y_pred)
      scores["Classif_Train"][grp].append(trainscore)

      # Test the model on hold-out group

      y_pred = clf.predict(hold_out_set_X)
      testscore = scoreFn(hold_out_set_y, y_pred)
      scores["Classif_Test"][grp].append(testscore)

      print(
          f"Hold-out: {grp} | Pretrained model: CV: {initialScore}, Classification on train: {trainscore}, Classification on train: {testscore}")

      '''
      3. Add datapoints using query selection method until user-defined proportion of datapoints are in dataset.
      '''

      # Pick an unseen instance using uncertainty sampling to add to training data and perform CV
      curr_state = X_pretrain.shape[0]
      batch = 1
      while curr_state < stop_size:

        remaining = stop_size - curr_state

        if batch % 5 == 0:
          print(
              f"Seed: {s} | Hold-out: {grp} | Progress: {curr_state}/{stop_size} | Current training score: {trainscore} | Current testing score: {testscore}")

        # Pick a data point to add to training set based on query selection method. This point is based on the index of the current matrix X_pool, not original data matrix.
        if queryMethod == "Uncertainty":
          nextPoint = uncertainty_sampling(clf, X_pool, batch_size=batch_size)

        elif queryMethod == "Diversity":
          nextPoint = diversity_k_means(X_pool, batch_size)

        elif queryMethod == "QBC":
          ensemble = clf.estimators_
          nextPoint = query_by_committee(ensemble, X_pool, batch_size)

        elif queryMethod == "Random":
          nextPoint = np.random.choice(
              list(range(X_pool.shape[0])), size=batch_size, replace=False)  # random sampling

        next_X = X_pool[nextPoint, :]
        next_y = y_pool[nextPoint]
        next_group = groups_pool[nextPoint]

        # Add to training dataset
        if len(next_X.shape) == 1:
          X_pretrain = np.append(X_pretrain, next_X.reshape((1, -1)), axis=0)
        else:
          X_pretrain = np.append(X_pretrain, next_X, axis=0)
        y_pretrain = np.append(y_pretrain, next_y)
        groups_pretrain = np.append(groups_pretrain, next_group)

        # Remove from unseen pool
        X_pool = np.delete(X_pool, nextPoint, axis=0)
        y_pool = np.delete(y_pool, nextPoint)
        groups_pool = np.delete(groups_pool, nextPoint)

        # Train model again with 5-fold CV
        CVscore = gshuffle_cross_validator(
            scoreFn, X_pretrain, y_pretrain, groups_pretrain, s)
        scores["CV"][grp].append(CVscore)

        if X_pool.shape[0] == 0:
          # No more unseen observations left
          break
        clf = RandomForestClassifier(
            random_state=s, n_estimators=1000, n_jobs=12)

        # Train model on all pretraining data and evaluate on unseen training data
        clf.fit(X_pretrain, y_pretrain)
        y_pred = clf.predict(X_pool)
        trainscore = scoreFn(y_pool, y_pred)
        scores["Classif_Train"][grp].append(trainscore)

        # Test the model on hold-out group

        y_pred = clf.predict(hold_out_set_X)
        testscore = scoreFn(hold_out_set_y, y_pred)
        scores["Classif_Test"][grp].append(testscore)

        curr_state = X_pretrain.shape[0]
        batch += 1

    return scores

  X = np.concatenate((protein, ligand), axis=1)
  print(f"Input size: {X.shape} | Target size: {y.shape}")

  assert end_prop > start_prop

  ############
  # Uncomment below to run in parallel
  ############

  # results = Parallel(n_jobs=3)(delayed(single_seed)(s, X, y, groups, scoreFn)
  #                              for s in seeds)  # Run different seeds in parallel
  ############
  # Uncomment below to run in serial
  ############

  results = []
  for s in seeds:
    res = single_seed(s, X, y, groups, scoreFn)
    results.append(res)

  # results: numSeeds-length list of scores dictionary with keys "CV", "Classif_Train" and "Classif_Test". Each is a dictionary with keys "CMGC", "TKL" and "TYR" and values as a numIterations-length list of scores. The list will be different sizes for different hold-out groups but same across seeds.

  allScores = {}
  allScores["Mean_CV"] = {key: [] for key in uniqueGroups}
  allScores["Std_CV"] = {key: [] for key in uniqueGroups}
  allScores["Mean_Classif_Train"] = {key: [] for key in uniqueGroups}
  allScores["Std_Classif_Train"] = {key: [] for key in uniqueGroups}
  allScores["Mean_Classif_Test"] = {key: [] for key in uniqueGroups}
  allScores["Std_Classif_Test"] = {key: [] for key in uniqueGroups}
  for grp in uniqueGroups:
    grpCVSeeds = np.array([results[i]["CV"][grp] for i in range(len(seeds))])
    grpClassifTrainSeeds = np.array(
        [results[i]["Classif_Train"][grp] for i in range(len(seeds))])
    grpClassifTestSeeds = np.array(
        [results[i]["Classif_Test"][grp] for i in range(len(seeds))])

    if queryMethod != None:

      # mean of different seeds --> single array of values
      grpCVMean = np.mean(grpCVSeeds, axis=0)
      grpCVStd = np.std(grpCVSeeds, axis=0)  # std of different seeds

      assert grpCVMean.shape[0] != len(seeds)  # should be numIterations-length

      grpClassifTrainMean = np.mean(grpClassifTrainSeeds, axis=0)
      grpClassifTrainStd = np.std(grpClassifTrainSeeds, axis=0)

      grpClassifTestMean = np.mean(grpClassifTestSeeds, axis=0)
      grpClassifTestStd = np.std(grpClassifTestSeeds, axis=0)

      allScores["Mean_CV"][grp] = grpCVMean
      allScores["Std_CV"][grp] = grpCVStd

      allScores["Mean_Classif_Train"][grp] = grpClassifTrainMean
      allScores["Std_Classif_Train"][grp] = grpClassifTrainStd

      allScores["Mean_Classif_Test"][grp] = grpClassifTestMean
      allScores["Std_Classif_Test"][grp] = grpClassifTestStd
    else:
      # Offline model scores
      # mean of different seeds --> single scalar value
      grpCVMean = np.mean(grpCVSeeds)
      grpCVStd = np.std(grpCVSeeds)  # std of different seeds

      grpClassifTrainMean = np.mean(grpClassifTrainSeeds)
      grpClassifTrainStd = np.std(grpClassifTrainSeeds)

      grpClassifTestMean = np.mean(grpClassifTestSeeds)
      grpClassifTestStd = np.std(grpClassifTestSeeds)

      allScores["Mean_CV"][grp] = grpCVSeeds
      allScores["Std_CV"][grp] = grpCVStd

      allScores["Mean_Classif_Train"][grp] = grpCVSeeds
      allScores["Std_Classif_Train"][grp] = grpClassifTrainStd

      allScores["Mean_Classif_Test"][grp] = grpClassifTestMean
      allScores["Std_Classif_Test"][grp] = grpClassifTestStd

  return allScores


dataDir = os.path.join(ROOTDIR, fileName)
seeds = [42, 3, 0]

queryMethod = [None, "Random", "Uncertainty", "Diversity", "QBC"]
batch_size = [200, 500]

results = []

for q in queryMethod:
  for b in batch_size:
    print(f"Running {q} for batch size {b}")
    scores = model_runner_logo(
        dataDir, seeds, scoreFn, queryMethod=q, batch_size=b)
    res = {"Query": q, "Batch_Size": b, "mean_CV": scores["Mean_CV"], "std_CV": scores["Std_CV"],
           "mean_classif_train": scores["Mean_Classif_Train"], "std_classif_train": scores["Std_Classif_Train"],
           "mean_classif_test": scores["Mean_Classif_Test"], "std_classif_test": scores["Std_Classif_Test"],
           "Shuffle": None, "Embedding": "Both", "Score": scoreName}
    results.append(res)

with open(f"Default_Run_logo_{scoreName}.pkl", "wb") as f:
  pickle.dump(results, f)
